{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name here.  \n",
    "Your Workshop section here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 5: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit this notebook to bCourses to receive a credit for this assignment.**\n",
    "\n",
    "Please complete this homework assignment in code cells in the iPython notebook. Include comments in your code when necessary. Enter your name in the cell at the top of the notebook, and rename the notebook [email_name]_HW05.ipynb, where [email_name] is the part of your UCB email address that precedes \"@berkeley.edu\". Please also submit a PDF of the jupyter notebook to bcourses.  Note, that when saving as PDF you don't want to use the option with latex because it crashes, but rather the one to save it directly as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Parity-Violating Asymmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data sample for this problem comes from the <a href=\"http://www.slac.stanford.edu/exp/e158\">E158</a> experiment at SLAC (a national lab near that Junior university across the Bay). E158 measured a parity-violating asymmetry in MÃ¸ller (electron-electron) scattering. See [this](https://en.wikipedia.org/wiki/Parity_(physics)) wikipedia article if you're interested to learn more about parity and parity violation. This was a fixed-target experiment, which scattered longitudinally-polarized electrons off atomic (unpolarized) electrons in the 1.5m liquid hydrogen target. The polarization of an electron refers to the relationship between the direction of its spin with respect to its momentum. Electrons can either be longtudinally polarized (spin parallel or antiparallel to the momentum) or transversely polarized (spin at right angles to the momentum).\n",
    "\n",
    "The data below contains a snapshot of 10,000 \"events\" from this experiment (in total, the experiment collected almost 400 million such events over the course of about 4 months). Each event actually records a pair of pulses: one for the right-handed electron (spin pointing along momentum) and one for the left-handed electron (spin pointing in the opposite direction to the momentum). For each event, we record 4 variables:\n",
    "\n",
    "* Counter: event index\n",
    "* Asymmetry: \"raw\" cross section asymmetry $A_{raw}$ from one of the detector channels (there are 50 of these overall). The cross section asymmetry is defined as \n",
    "$A_{raw} = \\frac{\\sigma_R-\\sigma_L}{\\sigma_R+\\sigma_L}$\n",
    "The asymmetry is recorded in units of PPM (parts per million). It is called \"raw\" because corrections due to the difference in beam properties at the target are not yet applied. The cross-section is a measure of the probability of a certain process occuring and relates directly to the number of events observed.\n",
    "* DeltaX: difference in beam position $\\Delta X=X_R-X_L$ at the target in X direction in microns (with the convention that the beam is traveling along Z)\n",
    "* DeltaY: difference in beam position $\\Delta Y=Y_R-Y_L$ at the target in Y direction in microns\n",
    "\n",
    "The data sample is provided in plain text format as the file <tt>asymdata.txt</tt>. Questions for this analysis:\n",
    "\n",
    "1. Read the data from the file, and plot distributions of $A_{raw}$, $\\Delta X$, and $\\Delta Y$. \n",
    "1. Compute the mean of the raw asymmetry distribution and its statistical uncertainty.\n",
    "1. Plot $A_{raw}$ vs $\\Delta X$, $A_{raw}$ vs $\\Delta Y$, and $\\Delta X$ vs $\\Delta Y$ as scatter plots. \n",
    "1. Compute the correlation coefficients <tt>Corr(Asym,DeltaX)</tt>, <tt>Corr(Asym,DeltaY)</tt>, and <tt>Corr(DeltaX,DeltaY)</tt>. See lecture notes, workshops, or https://en.wikipedia.org/wiki/Pearson_correlation_coefficient for additional help understanding correlation coefficients. Which variables are approximately independent of each other ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our standard import statements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "'''There are several different ways to change the style of your plots.  If you have a few\n",
    "   style choices that you'd like all figures to share, you can change the parameters in matplotlib's\n",
    "   resource (.rc) file.  The next two lines change the default figure and font sizes.  You can read\n",
    "   more about the available customization options here: \n",
    "   https://matplotlib.org/3.1.3/tutorials/introductory/customizing.html'''\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 8,4\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "'''First, take a look at the data file (asymdata.txt).  It should be in the same datahub directory\n",
    "   as this HW file; you can access your file browser by clicking the Jupyter logo at the top left of\n",
    "   the notebook.  (We're in the PHYSICS-88/sp20 directory this semester).  Note that the first line\n",
    "   of the text file contains the column labels.  If you use numpy's loadtxt() function to read in the\n",
    "   data file, the 'skiprows' option will be useful to skip over the first row.  You can also use the \n",
    "   'unpack = True' option to load the 4 columns of data into 4 separate numpy arrays'''\n",
    "\n",
    "# Load the data from the file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use matplotlib histograms to plot the distributions\n",
    "\n",
    "'''Remember to label your plots.  Use plt.figure() or plt.show() between\n",
    "   your plotting statements to plot the histograms on separate figures.\n",
    "   Alternatively, you can make the 3 histograms subplots of a common figure.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and its uncertainty (this is the standard error of the mean from Workshop 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the three scatter plots\n",
    "\n",
    "'''If you have two arrays 'A_values' and 'B_values', plt.scatter(A_values, B_values) makes\n",
    "   a scatter plot with B values on the y-axis and A values on the x-axis (B vs A)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40212859128196293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.40212859],\n",
       "       [0.40212859, 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pearson_correlation(x, y):\n",
    "    '''This function calculates and the returns the Pearson correlation\n",
    "       coefficient between two input arrays (x and y) of equal length.'''\n",
    "    \n",
    "    # Write your own function to find the correlation coefficient.  You can\n",
    "    # a combination of loops and any numpy function you'd like except for\n",
    "    # numpy.corrcoef(x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Determine which pairs of data columns are dependent (strong positive or negative correlation)\\n   or independent (not much correlation).  The values you find should agree with the scatter plots\\n   you made. You can also use np.corrcoef() to check the correctness of your correlation function.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now use your function to print the 3 correlation coefficients\n",
    "\n",
    "'''Determine which pairs of data columns are dependent (strong positive or negative correlation)\n",
    "   or independent (not much correlation).  The values you find should agree with the scatter plots\n",
    "   you made. You can also use np.corrcoef() to check the correctness of your correlation function.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Gamma-ray peak\n",
    "\n",
    "[Some of you may recognize this problem from Advanced Lab's Error Analysis Exercise. That's not an accident.]\n",
    "\n",
    "You are given a dataset (`peak.dat`) from a gamma-ray experiment consisting of ~1000 gamma-ray hits. Each line in the file corresponds to one recorded gamma-ray event, and stores the the measured energy of the gamma-ray (in MeV). We will assume that the energies are randomly distributed about a common mean, and that each event is uncorrelated to others. Read the dataset from the enclosed file and:\n",
    "1. Produce a histogram of the distribution of energies. Choose the number of bins wisely, i.e. so that the width of each bin is smaller than the width of the peak, and at the same time so that the number of entries in the most populated bin is relatively large. Since this plot represents randomly-collected data, plotting error bars would be appropriate.\n",
    "1. Compute the mean and standard deviation of the distribution of energies and their statistical uncertainties. Assume the distribution is Gaussian and see the lecture notes for the formulas for the mean and variance of the sample and the formulas for the errors on these quantities. \n",
    "1. Compute the fraction of events contained within $\\pm 1\\sigma$ of the mean, $\\pm 2\\sigma$ of the mean, and $\\pm 3\\sigma$ of the mean (where $\\sigma$ is the standard deviation you computed in Part 2). Compare these fractions with the quantiles of the Gaussian distribution (see lecture notes) ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 8,4\n",
    "plt.rcParams['font.size'] = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.loadtxt() to read-in the data file peak.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Unfortunately, plt.hist() does not have a built-in option to plot errors on top of the\n",
    "   histogram bars.  The work-around is to get the counts and bin edges from the np.histogram()\n",
    "   or plt.hist()---np.histogram() is preferred because this function doesn't actually make a plot.\n",
    "   After calculating your desired bin centers (using the bin edges), you can use plt.bar() to make\n",
    "   a bar graph, which has a 'yerr' option.  An example of this method can be found here:\n",
    "   https://stackoverflow.com/questions/11774822/matplotlib-histogram-with-errorbars'''\n",
    "\n",
    "# Plot your histogram (bar graph) with the appropriate labels and error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and its uncertainty (this is the standard error of the mean from Workshop 5)\n",
    "# Also compute the standard deviation and its uncertainty (see the lecture notes for the uncertainty formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute the fraction of counts within 1, 2, and 3 standard deviations of the mean\n",
    "'''Within one standard deviation is any count with energy greater than (mean - sigma)\n",
    "   and less than (mean + sigma).  You can use loops to calculate these fractions, or a \n",
    "   combination of np.sum() and np.all() (although you'd have to look up these functions first\n",
    "   to understand why they'd be useful).'''\n",
    "\n",
    "# When comparing to the quantiles of the Gaussian distribution, it is helpful to be familiar\n",
    "# with the 68.7-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
