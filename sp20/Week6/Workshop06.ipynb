{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 6: Fitting\n",
    "\n",
    "**Submit this notebook to bCourses to receive a grade for this Workshop.**\n",
    "\n",
    "Please complete workshop activities in code cells in this iPython notebook. The activities titled **Practice** are purely for you to explore Python, and no particular output is expected. Some of them have some code written, and you should try to modify it in different ways to understand how it works. Although no particular output is expected at submission time, it is _highly_ recommended that you read and work through the practice activities before or alongside the exercises. However, the activities titled **Exercise** have specific tasks and specific outputs expected. Include comments in your code when necessary. Enter your name in the cell at the top of the notebook. \n",
    "\n",
    "**The workshop should be submitted on bCourses under the Assignments tab (both the .ipynb and .pdf files).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fitting\n",
    "\n",
    "The simplest technique for fitting is called least-squares fitting. Usually you use the least-squares fit if you have a set of data (pairs of data points $(x_i, y_i)$ ), and you want to describe it in terms of a model $y(x;\\{a_j\\})$, where parameters $\\{a_j\\}$ are unknown. The purpose of your fit is to determine the values of $\\{a_j\\} = \\{a_0, a_1, ...\\}$ and (hopefully) their uncertainties. An example of a model is:\n",
    "\n",
    "$$y = a_0 + a_1 x$$\n",
    "\n",
    "where the unknown parameters $a_j$ are $a_0$ and $a_1$.\n",
    "\n",
    "There are two standard cases where least-squares method is applicable:\n",
    "1. You know errors for each data point $\\sigma_i$ and you know that those errors follow a Gaussian distribution. In this case, you minimize $\\chi^2=\\sum \\left(\\frac{y_i - y(x_i;\\{a_j\\})}{\\sigma_i}\\right)^2$ with respect to the parameters $\\{a_j\\}$. The value of the $\\chi^2_{\\min}$ can be interpreted as a goodness-of-fit. The parameters $\\{a_j\\}$ that minimize the $\\chi^2$ have a probabilistic interpretation.\n",
    "1. You know that the errors are Gaussian and are the same for each data point, but you do not know their magnitude. In this case, you would minimize the sum of squares: $\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\{a_j\\})\\right)^2$. Then value of $\\mathcal{S}$ can be used to *estimate* the errors $\\sigma_i$ for each data point using the following formula: $\\sigma_i = \\sqrt{\\mathcal{S}/(N_\\mathrm{data}-N_\\mathrm{parameters})}$\n",
    "The errors on $\\{a_j\\}$ have a probabilistic definition, but you lose information about the goodness of fit.\n",
    "1. If the errors are not known to be Gaussian, then the least square method is not useful to estimate uncertainties or the goodness of fit. It is also not guaranteed to be unbiased or most efficient (see the lecture for a discussion of these properties of estimators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial $\\chi^2$ fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as fitter\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate artificial data = straight line with a=0 and b=1\n",
    "# plus some noise.\n",
    "a0 = 0\n",
    "a1 = 1\n",
    "sig = 0.3\n",
    "Npoints = 10\n",
    "\n",
    "xdata = np.arange(0,Npoints,1.)\n",
    "ydata = a0 + a1*xdata + sig*np.random.standard_normal(size=Npoints)\n",
    "sigma = np.ones(Npoints)*sig\n",
    "plt.scatter(xdata,ydata,color='b')\n",
    "plt.errorbar(xdata,ydata, sigma, color='r',ls='none')\n",
    "plt.xlim(-1,Npoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform fitting using `scipy.optimize.curve_fit()`, we'll need to define a model function.  The function must take on a specific form.  The first input variable must be the x data values, followed by the parameters we are trying to fit ($a_0, a_1, ...$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a0, a1):\n",
    "    return a0 + a1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also standard to give `scipy.optimize.curve_fit()` an initial starting point for the free parameters $\\{a_j\\}$.  We created the data, so we know $a_0 \\approx 0$ and $a_1 \\approx 1$.  Typically you would look a plot of the data to pick reasonable starting points for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par0 = np.array([0,1]) # initial guess for parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `scipy.optimize.curve_fit()`, you need to at least give it the model function (`model`), a list/array of x data values (`xdata`), and a list/array of y data values (`ydata`).  It is standard to give a starting point for the parameter search (`par0`), and errors on the y data values (`sigma`) if you know them.  Finally, you'll want to throw an `absolute_sigma = True` at the end.  (The alternative `absolute_sigma = False` is used when you only know the *relative* magnitudes of the y errors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par, cov = fitter.curve_fit(model, xdata, ydata, par0, sigma, absolute_sigma=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fitting function returns two things, which we are now calling `par` and `cov`.  The first is an array of the best-fit values (the ones that minimize $\\chi^2$) for $a_0$ and $a_1$.  The second is something called a covariance matrix, which contains an error estimate for each parameter on the diagonal elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best-fit value for a0 is the first element of the returned parameters\n",
    "a0 = par[0]\n",
    "# And the error estimate is the first diagonal element (row 0, column 0) of the covariance matrix\n",
    "error_a0 = np.sqrt(cov[0,0])\n",
    "print('a0={0:6.3f}+/-{1:5.3f}'.format(a0, error_a0))\n",
    "\n",
    "\n",
    "# The best-fit value for a1 is the second element of the returned parameters\n",
    "a1 = par[1]\n",
    "# And the error estimate is the second diagonal element (row 1, column 1) of the covariance matrix\n",
    "error_a1 = np.sqrt(cov[1,1])\n",
    "print('a1={0:6.3f}+/-{1:5.3f}'.format(a1, error_a1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to check how well we did with the fit, it's typical to compute the $\\chi^2$ value.  Unfortunately, this doesn't come as an output of the `scipy.optimize.curve_fit()` function, but it's easy to do it on one line using `np.sum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(xdata, *par) is equivalent to model(xdata, par[0], par[1])\n",
    "\n",
    "chi_squared = np.sum(((model(xdata, *par)-ydata)/sigma)**2) \n",
    "print ('chi^2 = {0:5.2f}'.format(chi_squared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, the terms inside the sum are the differences between the fitted y values (`model(xdata, *par)`) and the actual y data values (`ydata`).  Each term is 'normalized' by our estimate on the y error (`sigma`) and then squared (you can think of the squaring as just a convenient way to make sure all the terms are positive).  \n",
    "\n",
    "If our estimates for the y errors are correct, then each term in the sum will be about 1 and $\\chi^2 \\approx N$ where $N$ is the number of data points.  It is standard to divide $\\chi^2$ by the number of *degrees of freedom*, which is the number of data points minus the number of free parameters in our fit.  If everything is done correctly, our reduced $\\chi^2$ value should be around 1.  A value much larger than 1 indicates a poor fit or an underestimate of y errors.  A value much smaller than 1 indicates that we are \"over-fitting\" the data (possibly using too many free parameters) or overestimating the y errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2/d.f.={0:5.2f}'.format(reduced_chi_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best fit line over the data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,Npoints)\n",
    "xfit = np.linspace(0,Npoints-1.,50)\n",
    "plt.plot(xfit,model(xfit, *par),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do an *unweighted* fit if you do not know the uncertainties for each point. *curve_fit* will minimize \n",
    "$\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\{a_j\\})\\right)^2$. You can use it to estimate the uncertainty for each point: \n",
    "$\\sigma_i = \\sqrt{\\mathcal{S}/(N_\\mathrm{data}-N_\\mathrm{parameters})}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par0 = np.array([0,1]) # initial guess for parameters\n",
    "par, cov = fitter.curve_fit(model, xdata, ydata, par0) # Now we don't know the errors\n",
    "\n",
    "a0 = par[0]\n",
    "error_a0 = np.sqrt(cov[0,0])\n",
    "print ('a={0:6.3f}+/-{1:5.3f}'.format(a0,error_a0))\n",
    "a1 = par[1]\n",
    "error_a1 = np.sqrt(cov[1,1])\n",
    "print ('b={0:6.3f}+/-{1:5.3f}'.format(a1,error_a1))\n",
    "\n",
    "# Compute the error per point\n",
    "sigCalc = np.sqrt(np.sum(((model(xdata, *par)-ydata))**2)/(len(xdata)-len(par)))\n",
    "print ('Generated error = {0:5.2f}'.format(sig))\n",
    "print ('Computed error = {0:5.2f}'.format(sigCalc))\n",
    "\n",
    "# Plot best fit line over the data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,Npoints)\n",
    "xfit = np.linspace(0,Npoints-1.,50)\n",
    "plt.plot(xfit,model(xfit,*par),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "The code in the cell immediately below will generate some data and store it in the arrays `xdata`, `ydata`, and `sigma`. Use the techniques above to a quadratic model of the form\n",
    "$$y = a_0 + a_1 x + a_2 x^2$$\n",
    "\n",
    "Plot the data and your best fit curve with error and print out the values and their uncertainties as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate artificial data for Exercise 6 and plot it\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as fitter\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate artificial data = quadratic function with a0 = 0, a1=1, a2 = -0.3\n",
    "# plus some noise.\n",
    "a0 = 0.5\n",
    "a1 = 1\n",
    "a2 = -0.3\n",
    "sig = 0.4\n",
    "Npoints = 10\n",
    "\n",
    "xdata = np.arange(0,Npoints,1.)\n",
    "ydata = a0 + a1 * xdata + a2 * xdata **2 + sig * np.random.standard_normal(size=Npoints)\n",
    "sigma = np.ones(Npoints)*sig\n",
    "plt.scatter(xdata,ydata,color='b')\n",
    "plt.errorbar(xdata,ydata, sigma, color='r',ls='none')\n",
    "plt.xlim(-1,Npoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 1\n",
    "\n",
    "def model(x, a0, a1, a2):\n",
    "    '''Define your model function according to the\n",
    "       quadratic function given above.'''\n",
    "    \n",
    "# Use scipy.optimize.curve_fit() to fit the generated data and print the results\n",
    "# Plot your fitted function over the data points to confirm you achieved a good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to a distribution\n",
    "\n",
    "`scipy.stats` package provides parameterizations of many standard probability density functions (PDFs). Each PDF has a *fit()* method, which does an *unbinned maximum likelihood fit* to a set of events, constraining the parameters of the PDF. Here is an example of a fit of a set of events to a Gaussian PDF.\n",
    "Courtesy http://glowingpython.blogspot.com/2012/07/distribution-fitting-with-scipy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm # This gives us access to the normal distribution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generate 1000 events from a normal distrubution\n",
    "# with actual mean 0 and actual standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=1000) \n",
    "\n",
    "# fit this sample to a gaussian distribution, determine parameters (stored in par)\n",
    "par = norm.fit(sample) \n",
    "\n",
    "print ('mean  = {0:4.2f}'.format(par[0]))\n",
    "print ('sigma = {0:4.2f}'.format(par[1]))\n",
    "\n",
    "# now, par[0] and par[1] are the mean and \n",
    "# the standard deviation of the fitted distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and our fitted curve to visualize how well it fit\n",
    "\n",
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "# fitted distribution\n",
    "pdf_fitted = norm.pdf(x,loc=par[0],scale=par[1])\n",
    "\n",
    "plt.title('Normal distribution')\n",
    "plt.plot(x,pdf_fitted,'r-')\n",
    "plt.hist(sample,density=True, alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, *scipy.stats.fit* does not provide an esitimate of the covariance matrix (or even the errors on the parameters). This is a major problem with doing max-likelihood fits using the *scipy.stats* package! (any respectable physicist needs to report errors for their measurements). There are more advanced tools for likelihood fitting that do provide error estimates -- but at this point they are beyond the scope of this class.\n",
    "\n",
    "For a Gaussian distribution, and a few others, max-likelihood estimators have analytic formulae:\n",
    "\n",
    "$\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "\n",
    "$\\hat{\\sigma^2} = \\frac{1}{N}\\sum_{i=1}^N (x_i-\\hat{\\mu})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 events from a normal distrubution\n",
    "# with actual mean 0 and actual standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=1000) \n",
    "\n",
    "N = len(sample)\n",
    "\n",
    "# These are the estimates for the mean and standard deviation given above\n",
    "mean = np.mean(sample)\n",
    "sigma = np.sqrt(np.sum((sample-mean)**2)/N)\n",
    "\n",
    "# The estimates on the errors for the mean and standard deviation can be found in most statistics textbooks\n",
    "eMean = sigma/np.sqrt(N)\n",
    "\n",
    "# See https://stats.stackexchange.com/questions/156518/what-is-the-standard-error-of-the-sample-standard-deviation\n",
    "eSigma = np.sqrt((np.sum((sample-mean)**4)/N-(N-3)/(N-1)*sigma**4)/N)\n",
    "\n",
    "print ('Max-likelihood estimate of mean = {0:4.2f} +/- {1:4.2f}'.format(mean,eMean))\n",
    "print ('Max-likelihood estimate of sigma = {0:4.2f} +/- {1:4.2f}'.format(sigma,eSigma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common way to determine parameters of a distribution is to do perform a *binned least-squares fit*, i.e. fit a distribution to a histogram. When doing so, it is important to assign proper Poisson errors to each bin of a histogram. Here is how you can create a histogram with Poisson errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for Exercise 2\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generate 1000 events from a normal distrubution\n",
    "# with actual mean 0 and actual standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `np.histogram()` allows us to convert our sample of 1000 normally distributed numbers to x values and y values with which we can perform a fit.  In this case, the y values are the heights of bars on the histogram, and the x values are the center position of each histogram bin.  The `np.histogram()` function needs a range over which we place the sample values into bins, along with the number of bins we would like to define over that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.histogram() with our sample to generate an array of y_values\n",
    "# (The y values would be the heights of each bar in the histogram)\n",
    "y, bin_edges = np.histogram(sample, range=(-4,4),bins=40)\n",
    "\n",
    "# bin_edges is an array of left/right edges of all bins \n",
    "# (40 bins are described by 41 edges)\n",
    "x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "# x is now an array of the bin centers\n",
    "\n",
    "# Poisson errors are just the square root of the number of counts in each bin\n",
    "error_y = np.sqrt(y)\n",
    "# We have to be careful to make sure errors are never zero, or the chi^2 fit would fail! \n",
    "# This next line assigns a minimum error of 1 to all bins\n",
    "error_y = [max(error,1) for error in error_y]\n",
    "\n",
    "# Here's a plot of our data\n",
    "plt.errorbar(x,y,error_y, fmt='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the data above using `scipy.optimize.curve_fit()`. The data are stored in the arrays `x`, `y`, and `error_y` (the estimated uncertainties).  Define your model function to be a Gaussian function with the form\n",
    "$$ y(x) = Ae^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "\n",
    "Your model function should have 3 free parameters: `A`, `mu`, and `sigma`.  Print the mean and sigma parameters and their uncertainties after fitting. Plot your function overlayed with the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 2\n",
    "\n",
    "def model(x, A, mu, sigma):\n",
    "    '''Define your model function according to \n",
    "       the Gaussian function given above.'''\n",
    "    \n",
    "# Use scipy.optimize.curve_fit() to fit the generated data and print the results\n",
    "# Plot your fitted function over the data points to confirm you achieved a good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (useful for HW5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, 100 random (x,y) points are generated and plotted on a scatter plot.  You can change how *correlated* the x and y values are by adjusting the coefficient of the `0*data1` term.  Right now, `0` means there is no correlation.  In this exercise, let's look at a couple ways of quantifying correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as fitter\n",
    "\n",
    "data1 = np.random.randn(100)\n",
    "\n",
    "data2 = np.random.randn(100) + 0*data1\n",
    "\n",
    "plt.scatter(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as we've done before, use `scipy.optimize.curve_fit()` to fit the data to a line.  Then print the slope of the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a0, a1):\n",
    "    return a0 + a1*x\n",
    "\n",
    "# Use curve_fit() with data1 and data2 \n",
    "# Then print the fitted slope (a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *correlation coefficient* can be found using `np.corrcoef(data1, data2)`.  This function returns a matrix; the off diagonal elements give the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between `data1` and `data2`.  Compare this value to the slope found above and the overall structure of the scatter plot.  Go back and change the `0*data1` term to something non-zero and see how this affects the scatter plot, slope, and correlation coefficient.\n",
    "\n",
    "You don't have to type up any answers to these exploratory questions; the goal is to build some intuition for what it means for two sets of data to be correlated.  Also note that slopes and correlation coefficients are not the same thing.  The Pearson correlation coefficient is bounded between -1 and +1 (+1 indicates strong positive correlation and -1 indicates strong negative correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the Pearson correlation coefficient between data1 and data2\n",
    "# go back and change 0*data1 to something like 3*data1 or -0.5*data1 and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW5, you'll need to define your own correlation function.  If you have the time, do this now and you can just copy it into your homework notebook later.  The Pearson correlation coefficient of a sample of (x,y) data points is defined as:\n",
    "\n",
    "$r_{xy} =\\frac{\\sum ^n _{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum ^n _{i=1}(x_i - \\bar{x})^2} \\sqrt{\\sum ^n _{i=1}(y_i - \\bar{y})^2}}$\n",
    "\n",
    "Where $\\bar{x}$ and $\\bar{y}$ are the means of the $x$ and $y$ data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
