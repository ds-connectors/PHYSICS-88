{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 7: Statistics\n",
    "\n",
    "**Submit this notebook to bCourses to receive a grade for this Workshop.**\n",
    "\n",
    "Please complete workshop activities in code cells in this iPython notebook. The activities titled **Practice** are purely for you to explore Python. Some of them may have some code written, and you should try to modify it in different ways to understand how it works. Although no particular output is expected at submission time, it is _highly_ recommended that you read and work through the practice activities before or alongside the exercises. However, the activities titled **Exercise** have specific tasks and specific outputs expected. Include comments in your code when necessary. Enter your name in the cell at the top of the notebook. The workshop should be submitted on bCourses under the Assignments tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview: generating random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss simulations in greater detail later in the semester. The first step in simulating nature -- which, despite Einstein's objections, is playing dice after all -- is to learn how to generate some numbers that appear random. Of course, computers cannot generate true random numbers -- they have to follow an algorithm. But the algorithm may be based on something that is difficult to predict (e.g. the time of day you are executing this code) and therefore *look* random to a human. Sequences of such numbers are called *pseudo-random*. \n",
    "\n",
    "The random variables you generate will be distributed according to some *Probability Density Function* (PDF). The most common PDF is *flat*: $f(x)=\\frac{1}{b-a}$ for $x\\in [a..b)$. Here is how to get a random number uniformly distributed between $a=0$ and $b=1$ in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard preamble\n",
    "import numpy as np\n",
    "import scipy as sp      \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one random number between [0,1)\n",
    "x = np.random.rand()\n",
    "print ('x=', x)\n",
    "\n",
    "# generate an array of 10 random numbers between [0,1)\n",
    "array = np.random.rand(10)\n",
    "print (array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate a set of randomly-distributed integer values instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0,1000,10)  \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d distributions\n",
    "\n",
    "## Moments of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's SciPy library contains a set of standard statistical functions. See a few examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of data and compute mean and variance\n",
    "# This creates an array of 100 elements, uniformly-distributed between 100 and 200\n",
    "\n",
    "# Try changing the size parameter!\n",
    "x = np.random.uniform(low=100,high=200,size=100)\n",
    "\n",
    "print(x[0:10])\n",
    "# make a histogram\n",
    "n, bins, patches = plt.hist(x, 20)\n",
    "\n",
    "# various measures of \"average value\":\n",
    "print('Mean = {0:5.0f}'.format(sp.mean(x)))\n",
    "print( 'Median = {0:5.0f}'.format(sp.median(x)))\n",
    "\n",
    "# measure of the spread\n",
    "print('Standard deviation = {0:5.1f}'.format(np.std(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "We just introduced some new functions: `np.random.rand()`, `np.random.uniform()`, `plt.hist()`, `sp.mean()`, and `sp.median()`. So let's put them to work. You may also find `np.cos()`, `np.sin()`, and `np.std()` useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate 100 random numbers, uniformly distributed between [-$\\pi,\\pi$)\n",
    "1. Plot them in a histogram.\n",
    "1. Compute mean and standard deviation (RMS) \n",
    "1. Plot a histogram of sin(x) and cos(x), where x is a uniformly distributed random number between [-$\\pi$,$\\pi$). Do you understand this distribution ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian/Normal distribution\n",
    "\n",
    "You can also generate Gaussian-distributed numbers. Remember that a Gaussian (or Normal) distribution is a probability distribution given by \n",
    "\n",
    "$$P(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "where $\\mu$ is the average of the distribution and $\\sigma$ is the standard deviation. The **standard** normal distribution is a special case with $\\mu = 0$ and $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a single random number, gaussian-distributed with mean=0 and sigma=1. This is also called \n",
    "# a standard normal distribution\n",
    "x = np.random.standard_normal()\n",
    "print (x)\n",
    "\n",
    "# generate an array of 10 such numbers\n",
    "a = np.random.standard_normal(size=10)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "We now introduced `np.random.standard_normal()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate $N=100$ random numbers, Gaussian-distributed with $\\mu=0$ and $\\sigma=1$. \n",
    "1. Plot them in a histogram. \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n",
    "1. Compute the median of this distribution\n",
    "1. Now find the means, standard deviations, and errors on the means for each of the $M=1000$ experiments of $N=100$ measurements each. Plot a histogram of the means. Is it consistent with your calculation of the error on the mean for $N=100$ ? About how many experiments yield a result within $1\\sigma$ of the true mean of 0 ? About how many are within $2\\sigma$ ? Is this what you expected?\n",
    "1. Now repeat question 4 for $N=10,50,1000,10000$. Plot a graph of the RMS of the distribution of the means vs $N$. Is it consistent with your expectations ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential distribution\n",
    "\n",
    "In this part we will repeat the above process, but now using lists of exponentially distributed random numbers. The probability of selecting a random number between $x$ and $x+dx$ is $\\propto e^{-x}dx$. Exponential distributions often appear in lossy systems, e.g. if you plot an amplitude of a damped oscillator as a function of time. Or you may see it when you plot the number of decays of a radioactive isotope as a function of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a single random number, exponentially-distributed with scale=1. \n",
    "x = np.random.exponential()\n",
    "print (x)\n",
    "\n",
    "# generate an array of 10 such numbers\n",
    "a = np.random.exponential(size=10)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "We now introduced `np.random.exponential()`. This function can take up to two keywords, one of which is `size` as shown above. The other is `scale`. Use the documentation and experiment with this exercise to see what it does.\n",
    "\n",
    "1. What do you expect to be the mean of the distribution? What do you expect to be the standard deviation?\n",
    "1. Generate $N=100$ random numbers, exponentially-distributed with the keyword `scale` set to 1. \n",
    "1. Plot them in a histogram. \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n",
    "1. Now find the means, standard deviations, and errors on the means for each of the $M=1000$ experiments of $N=100$ measurements each. Plot a histogram of each quantity. Is the RMS of the distribution of the means consistent with your calculation of the error on the mean for $N=100$ ? \n",
    "1. Now repeat question 5 for $N=10,100,1000,10000$. Plot a graph of the RMS of the distribution of the means vs $N$. Is it consistent with your expectations ? This is a demonstration of the *Central Limit Theorem*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial distribution\n",
    "The binomial distribution with parameters $n$ and $p$ is the *discrete* probability distribution of the number of successes in a sequence of $n$ independent yes/no experiments, each of which yields success with probability $p$. A typical example is a distribution of the number of *heads* for $n$ coin flips ($p=0.5$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulates flipping 1 fair coin  one time. Returns 0 for heads and 1 for tails\n",
    "p = 0.5\n",
    "print (np.random.binomial(1,p))\n",
    "\n",
    "# Simulates flipping 5 biased coins three times\n",
    "p = 0.7\n",
    "print (np.random.binomial(5,p, size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "We now introduced the function `np.random.binomial(n,p)` which requires two arguments, `n` the number of coins being flipped in a single trial and `p` the probability that a particular coin lands tails. As usual, `size` is another optional keyword argument.\n",
    "1. Generate an array of outcomes for flipping 1 unbiased coin 10 times.\n",
    "1. Plot the outcomes in a histogram (0=heads, 1=tails). \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson distribution\n",
    "The Poisson distribution is a *discrete* probability distribution that expresses the probability of a given number of events $n$ occurring in a fixed interval of time $T$ if these events occur with a known average rate $\\nu/T$ and independently of the time since the last event. The *expectation value* of $n$ is $\\nu$. The variance of $n$ is also $\\nu$, so the standard deviation of $n$ is $\\sigma(n) = \\sqrt{\\nu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 10 # expected number of events\n",
    "n = np.random.poisson(nu)  # generate a Poisson-distributed number.\n",
    "print (n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "We introduced `np.random.poisson()`. As usual, you can use the keyword argument `size` to draw multiple samples.\n",
    "1. Generate $N=100$ random numbers, Poisson-distributed with $\\nu=10$. \n",
    "1. Plot them in a histogram. \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n",
    "1. Now repeat question 3 for $\\nu=1,5,100,10000$. Plot a graph of the RMS vs $\\nu$. Is it consistent with your expectations ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fitting\n",
    "\n",
    "Examples of how to do a fit to a graph were given in lecture (see Lecture05.ipynb). Let's practice with the fits a bit more. \n",
    "\n",
    "The simplest technique to describe is least-squares fitting (see lecture notes). Usually you use the least-squares fit if you have a set of data (pairs of data points $(x_i, y_i)$ ), and you want to describe it in terms of a model $y(x;\\{\\theta_j\\})$, where parameters $\\{\\theta_j\\}$ are unknown. The purpose of your fit is to determine the values of $\\{\\theta_j\\}$ and (hopefully) their uncertainties. An example of a model is:\n",
    "\n",
    "$$y = a_0 + a_1 x$$\n",
    "\n",
    "where the unknown parameters $\\theta_j$ are $a_0$ and $a_1$.\n",
    "\n",
    "There are two standard cases where least-squares method is applicable:\n",
    "1. You know errors for each data point $\\sigma_i$ and you know that those errors are Gaussian. In this case, you minimize $\\chi^2=\\sum \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2$ with respect to the parameters $\\{\\theta_j\\}$. The value of the $\\chi^2_{\\min}$ can be interpreted as a goodness-of-fit. The parameters $\\{\\theta_j\\}$ that minimize $\\chi^2$ have probabilistic interpretation\n",
    "1. You know that the errors are Gaussian and are the same for each data point, but you do not know their magnitude. In this case, you would minimize the sum of squares: $\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\theta)\\right)^2$. Then value of $\\mathcal{S}$ can be used to *estimate* the errors $\\sigma_i$ for each data point: $\\sigma_i = \\sqrt{\\mathcal{S}/(N_\\mathrm{data}-N_\\mathrm{parameters})}$\n",
    "The errors on $\\theta$ have a probabilistic definition, but you lose information about the goodness of fit\n",
    "1. If the errors are not known to be Gaussian, then the least square method is not useful to estimate uncertainties or the goodness of fit. It is also not guaranteed to be unbiased or most efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial $\\chi^2$ fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as fitter\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate artificial data = straight line with a=0 and b=1\n",
    "# plus some noise.\n",
    "a0 = 0\n",
    "b0 = 1\n",
    "sig = 0.2\n",
    "Npoints = 10\n",
    "\n",
    "xdata = np.arange(0,Npoints,1.)\n",
    "ydata = a0+xdata*b0+sig*np.random.standard_normal(size=Npoints)\n",
    "sigma = np.ones(Npoints)*sig\n",
    "plt.scatter(xdata,ydata,color='b')\n",
    "plt.errorbar(xdata,ydata, sigma, color='r',ls='none')\n",
    "plt.xlim(-1,Npoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you run the cell above this first to generate some artificial data\n",
    "# to which we will fit this curve\n",
    "\n",
    "# Define a fit model. For this part, we will use a linear function\n",
    "# The function which defines your model HAS TO TAKE ON A SPECIFIC FORM\n",
    "def model(x, a, b):\n",
    "    return a + b*x\n",
    "\n",
    "# You have to supply an initial guess of parameters, and they should be \"close enough\" to the true values, otherwise\n",
    "# the fit may fall into a false minimum\n",
    "par0    = np.array([0.5, -0.3]) # initial guess for parameters\n",
    "par, cov = fitter.curve_fit(model, xdata, ydata, par0, sigma, absolute_sigma=True)\n",
    "\n",
    "# par arrays contains the values of parameters. cov is the covariance matrix\n",
    "# decode it now\n",
    "a = par[0]\n",
    "ea = np.sqrt(cov[0,0])\n",
    "print('a={0:6.3f}+/-{1:5.3f}'.format(a,ea))\n",
    "b = par[1]\n",
    "eb = np.sqrt(cov[1,1])\n",
    "print('b={0:6.3f}+/-{1:5.3f}'.format(b,eb))\n",
    "\n",
    "# compute reduced chi2\n",
    "chi_squared = np.sum(((model(xdata, *par)-ydata)/sigma)**2)\n",
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2 = {0:5.2f}'.format(chi_squared))\n",
    "print ('chi^2/d.f.={0:5.2f}'.format(reduced_chi_squared))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,Npoints)\n",
    "xfit = np.linspace(0,Npoints-1.,50)\n",
    "plt.plot(xfit,model(xfit,par[0],par[1]),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do an *unweighted* fit if you do not know the uncertainties for each point. *curve_fit* will minimize \n",
    "$\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\theta)\\right)^2$. You can use it to estimate the uncertainty for each point: \n",
    "$\\sigma_i = \\sqrt{\\mathcal{S}/(N_\\mathrm{data}-N_\\mathrm{parameters})}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to supply an initial guess of parameters, and they should be \"close enough\" to the true values, otherwise\n",
    "# the fit may fall into a false minimum\n",
    "par0    = np.array([0.5, -0.3]) # initial guess for parameters\n",
    "par, cov = fitter.curve_fit(model, xdata, ydata, par0, sigma, absolute_sigma=False)\n",
    "\n",
    "# par arrays contains the values of parameters. cov is the covariance matrix\n",
    "# decode it now\n",
    "a = par[0]\n",
    "ea = np.sqrt(cov[0,0])\n",
    "print ('a={0:6.3f}+/-{1:5.3f}'.format(a,ea))\n",
    "b = par[1]\n",
    "eb = np.sqrt(cov[1,1])\n",
    "print ('b={0:6.3f}+/-{1:5.3f}'.format(b,eb))\n",
    "\n",
    "# compute the error per point\n",
    "sigCalc = np.sqrt(np.sum(((model(xdata, *par)-ydata))**2)/(len(xdata)-len(par)))\n",
    "print ('Generated error = {0:5.2f}'.format(sig))\n",
    "print ('Computed error = {0:5.2f}'.format(sigCalc))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,Npoints)\n",
    "xfit = np.linspace(0,Npoints-1.,50)\n",
    "plt.plot(xfit,model(xfit,par[0],par[1]),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "We now introduced a new function: `scipy.optimize.curve_fit()`. The code in the cell immediately below will generate some data and store it an array of size `(500,3)` where the first column is $x$ values, the second column is the $y$ values, the third column is the uncertainty in each value. Use the techniques above to a quadratic model of the form\n",
    "$$y = a_0 + a_1 x + a_2 x^2$$\n",
    "\n",
    "Plot the data and your best fit curve with error and print out the values and their uncertainties as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate artificial data for Exercise 6 and plot it\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as fitter\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate artificial data = quadratic function with a0 = 0, a1=1, a2 = -0.3\n",
    "# plus some noise.\n",
    "a0 = 0.5\n",
    "a1 = 1\n",
    "a2 = -0.3\n",
    "sig = 0.4\n",
    "Npoints = 10\n",
    "\n",
    "xdata = np.arange(0,Npoints,1.)\n",
    "ydata = a0 + a1 * xdata + a2 * xdata **2 + sig * np.random.standard_normal(size=Npoints)\n",
    "sigma = np.ones(Npoints)*sig\n",
    "plt.scatter(xdata,ydata,color='b')\n",
    "plt.errorbar(xdata,ydata, sigma, color='r',ls='none')\n",
    "plt.xlim(-1,Npoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to a distribution\n",
    "\n",
    "`scipy.stats` package provides parameterizations of many standard probability density functions (PDFs). Each PDF has a *fit()* method, which does an *unbinned maximum likelihood fit* to a set of events, constraining the parameters of the PDF. Here is an example of a fit of a set of events to a Gaussian PDF.\n",
    "Courtesy http://glowingpython.blogspot.com/2012/07/distribution-fitting-with-scipy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generate 1000 events from a normal distrubution\n",
    "# with actual mean 0 and actual standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=1000) \n",
    "\n",
    "# fit this sample to a gaussian distribution, determine parameters (stored in par)\n",
    "par = norm.fit(sample) \n",
    "\n",
    "print ('mean={0:4.2f}'.format(par[0]))\n",
    "print ('sigma={0:4.2f}'.format(par[1]))\n",
    "\n",
    "# now, par[0] and par[1] are the mean and \n",
    "# the standard deviation of the fitted distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and our fitted curve to visualize how well it fit\n",
    "\n",
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "# original distribution that we generated data from\n",
    "pdf = norm.pdf(x)\n",
    "\n",
    "# fitted distribution\n",
    "pdf_fitted = norm.pdf(x,loc=par[0],scale=par[1])\n",
    "\n",
    "plt.title('Normal distribution')\n",
    "plt.plot(x,pdf_fitted,'r-',x,pdf,'b-')\n",
    "plt.hist(sample,density=True, alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, *scipy.stats.fit* does not provide an esitimate of the covariance matrix (or even the errors on the parameters). This is a major problem with doing max-likelihood fits using *scipy.stats* package ! (any respectable physicist needs to report errors for his/her measurements !). There are more advanced tools for likelihood fitting that do provide error estimates -- but at this point they are beyond the scope of this class. But here are a few pointers:\n",
    "\n",
    "- PyROOT: https://root.cern.ch/pyroot\n",
    "- KaFE: http://www-ekp.physik.uni-karlsruhe.de/~quast/kafe/html/\n",
    "\n",
    "For a Gaussian distribution, and a few others, max-likelihood estimators have analytic formulae:\n",
    "\n",
    "$\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "\n",
    "$\\hat{\\sigma^2} = \\frac{1}{N}\\sum_{i=1}^N (x_i-\\hat{\\mu})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 events from a normal distrubution\n",
    "# with actual mean 0 and actual standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=1000) \n",
    "\n",
    "N=len(sample)\n",
    "mean = np.mean(sample)\n",
    "sigma = np.sqrt(np.sum((sample-mean)**2)/N)\n",
    "eMean = sigma/np.sqrt(N)\n",
    "eSigma=np.sqrt((np.sum((sample-mean)**4)/N-(N-3)/(N-1)*sigma**4)/N)\n",
    "print ('Max-likelihood estimate of mean={0:4.2f}+/-{1:4.2f}'.format(mean,eMean))\n",
    "print ('Max-likelihood estimate of sigma={0:4.2f}+/-{1:4.2f}'.format(sigma,eSigma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common way to determine parameters of a distribution is to do perform a *binned least-squares fit*, i.e. fit a distribution to a histogram. When doing so, it is important to assign proper Poisson errors to each bin of a histogram. Here is how you can create a histogram with Poisson errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for Exercise 7\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generate 1000 events from a normal distrubution\n",
    "# with actual mean 0 and actual standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=1000) \n",
    "\n",
    "y, bin_edges = np.histogram(sample, range=(-4,4),bins=40)\n",
    "x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "ey = np.sqrt(y)\n",
    "# have to be careful to make sure errors are never zero, or chi2 fit would fail ! Assign minimum error of 1 to all bins\n",
    "ey = [max(error,1) for error in ey]\n",
    "\n",
    "plt.errorbar(x,y,ey, fmt='o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Fit the data above, stored in the arrays `x`, `y`, and `ey` (the estimated uncertainties), to a Gaussian function using a $\\chi^2$ fit, print mean and sigma parameters and their uncertainties. Plot your function overlayed with the data as plotted above. *Hint*: use *scipy.optimize.curve_fit* example above. A Gaussian function has the form\n",
    "$$ y(x) = Ae^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "How many parameters should your model function have? Remember that your final Gaussian function should be normalized (that is, it is a probability distribution, so the area under it should be equal to 1. How can you ensure that? *Hint*: The normal distribution defined above is normalized) For plotting, you may find the `norm.pdf()` function from `scipy.stats` (described above) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
